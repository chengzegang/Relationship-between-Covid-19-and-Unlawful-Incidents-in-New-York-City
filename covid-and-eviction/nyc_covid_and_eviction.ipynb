{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "educational-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "great-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Initialize SparkSession #########################\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkSQL\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-wisconsin",
   "metadata": {},
   "source": [
    "## Import data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parliamentary-fireplace",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/dannysongyd/NYU/big-data-project/data/eviction_cases.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6983f72eedf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# load cases and addresses tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m eviction_cases = spark.read.format('csv').options(\n\u001b[0m\u001b[1;32m      8\u001b[0m     header='true', inferschema='true').load(\"data/eviction_cases.csv\")\n\u001b[1;32m      9\u001b[0m eviction_addresses = spark.read.format('csv').options(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/Users/dannysongyd/NYU/big-data-project/data/eviction_cases.csv"
     ]
    }
   ],
   "source": [
    "######################### Import data set  #########################\n",
    "# cases has basic information about each case. (One row for each case)\n",
    "# addresses has address of the properties that the case concerns. (Can be one or more entries for each case)\n",
    "######################### Import oca_index data set  #########################\n",
    "\n",
    "# load cases and addresses tables\n",
    "eviction_cases = spark.read.format('csv').options(\n",
    "    header='true', inferschema='true').load(\"data/eviction_cases.csv\")\n",
    "eviction_addresses = spark.read.format('csv').options(\n",
    "    header='true', inferschema='true').load(\"data/eviction_addresses.csv\")\n",
    "\n",
    "# create temp view for spark sql \n",
    "eviction_cases.createOrReplaceTempView(\"eviction_cases\")\n",
    "eviction_addresses.createOrReplaceTempView(\"eviction_addresses\")\n",
    "\n",
    "# print schema\n",
    "eviction_cases.printSchema()\n",
    "eviction_addresses.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-chosen",
   "metadata": {},
   "source": [
    "## Total filling of eviction in NYC after NYC lockdown (03/20/2020)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 14,
>>>>>>> Stashed changes
   "id": "amazing-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Total filling of eviction in NYC after NYC lockdown (03/20/2020) #########################\n",
    "\n",
    "# These classifications are consider eviction fillings: Holdover, Non-Payment\n",
    "\n",
    "# These court are in the NYC: Bronx County Civil Court, Kings County Civil Court, New York County Civil Court, Queens County Civil Court, \n",
    "# Richmond County Civil Court, Redhook Community Justice Center and Harlem Community Justice Center\n",
    "\n",
    "# We only retreive the cases after 03/20/2020 when NYC declared a city lockdown \n",
    "\n",
    "\n",
    "\n",
    "######################### Total filling of eviction in NYC after NYC lockdown (03/20/2020) #########################\n",
    "query = \"\"\"\n",
    "select count(*)\n",
    "from eviction_cases\n",
    "where fileddate > '2020-03-20'\n",
    "  and classification in ('Holdover','Non-Payment')\n",
    "  and court in (\n",
    "\t\t\t\t\t'Bronx County Civil Court',\n",
    "\t\t\t\t\t'Kings County Civil Court',\n",
    "\t\t\t\t\t'New York County Civil Court',\n",
    "\t\t\t\t\t'Queens County Civil Court',\n",
    "\t\t\t\t\t'Richmond County Civil Court',\n",
    "\t\t\t\t\t'Redhook Community Justice Center',\n",
    "\t\t\t\t\t'Harlem Community Justice Center'\n",
    "\t\t\t\t)\n",
    "\n",
    "\"\"\"\n",
    "total_eviction_filling_after_lockdown = spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-correlation",
   "metadata": {},
   "source": [
    "## Per case filed date and disposed date"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "fantastic-pottery",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 15,
   "id": "fantastic-pottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------------+\n",
      "| fileddate|disposeddate|week_filed|week_disposed|\n",
      "+----------+------------+----------+-------------+\n",
      "|2020-03-24|        null|2020-03-23|         null|\n",
      "|2020-04-14|  2021-03-08|2020-04-13|   2021-03-08|\n",
      "|2020-04-14|        null|2020-04-13|         null|\n",
      "|2020-04-17|        null|2020-04-13|         null|\n",
      "|2020-05-12|        null|2020-05-11|         null|\n",
      "|2020-05-12|        null|2020-05-11|         null|\n",
      "|2020-05-12|        null|2020-05-11|         null|\n",
      "|2020-05-12|        null|2020-05-11|         null|\n",
      "|2020-06-15|        null|2020-06-15|         null|\n",
      "|2020-06-23|        null|2020-06-22|         null|\n",
      "|2020-06-23|        null|2020-06-22|         null|\n",
      "|2020-06-25|        null|2020-06-22|         null|\n",
      "|2020-06-25|        null|2020-06-22|         null|\n",
      "|2020-06-26|        null|2020-06-22|         null|\n",
      "|2020-06-26|        null|2020-06-22|         null|\n",
      "|2020-06-29|        null|2020-06-29|         null|\n",
      "|2020-06-30|        null|2020-06-29|         null|\n",
      "|2020-06-30|          27|2020-06-29|         null|\n",
      "|2020-06-30|        null|2020-06-29|         null|\n",
      "|2020-07-01|  2021-01-20|2020-06-29|   2021-01-18|\n",
      "+----------+------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "######################### case file date and disposed date #########################\n",
    "\n",
    "# Each row represents each eviction case's fileddate, disposeddate, week_filed and week_disposed\n",
    "# This is used for later data cleanning and integration\n",
    "\n",
    "######################### case file date and disposed date #########################\n",
    "query_after_lockdown_eviction_cases = \"\"\"\n",
    "select fileddate,\n",
    "       disposeddate,\n",
    "       cast(date_trunc('week', fileddate) as date)    as week_filed,\n",
    "       cast(date_trunc('week', disposeddate) as date) as week_disposed\n",
    "from eviction_cases\n",
    "where classification in ('Holdover', 'Non-Payment')\n",
    "  and court in ('Bronx County Civil Court',\n",
    "                'Kings County Civil Court',\n",
    "                'New York County Civil Court',\n",
    "                'Queens County Civil Court',\n",
    "                'Richmond County Civil Court',\n",
    "                'Redhook Community Justice Center',\n",
    "                'Harlem Community Justice Center')\n",
    "  and fileddate > '2020-03-20'\n",
    "  --and propertytype = 'Residential' # commented out to show Statewide evictions, which includes commercial\n",
    "order by fileddate asc\n",
    "\"\"\"\n",
    "after_lockdown_eviction_cases = spark.sql(query_after_lockdown_eviction_cases).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-rachel",
   "metadata": {},
   "source": [
    "## Per week case filed date and disposed date with running sum and total active cases"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 16,
>>>>>>> Stashed changes
   "id": "worst-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### case file date and disposed date with running sum and total active cases #########################\n",
    "\n",
    "# Each row represents each week (starting from 03/20/2020), the number of cases has been filed, the number of cases has been disposed,\n",
    "# the cumulative cases for each one\n",
    "# and the total active cases (filed_cases - disposed_cases)\n",
    "\n",
    "# Will draw a time series graph using this data\n",
    "\n",
    "######################### case file date and disposed date with running sum and total active cases #########################\n",
    "query_eviction_cases_time_and_summary = \"\"\"\n",
    "with after_lockdown as (\n",
    "    select fileddate,\n",
    "           cast(date_trunc('week', fileddate) as date)    as week_filed,\n",
    "           disposeddate,\n",
    "           cast(date_trunc('week', disposeddate) as date) as week_disposed\n",
    "    from eviction_cases\n",
    "    where classification in ('Holdover', 'Non-Payment')\n",
    "      and court in ('Bronx County Civil Court',\n",
    "                    'Kings County Civil Court',\n",
    "                    'New York County Civil Court',\n",
    "                    'Queens County Civil Court',\n",
    "                    'Richmond County Civil Court',\n",
    "                    'Redhook Community Justice Center',\n",
    "                    'Harlem Community Justice Center')\n",
    "      and fileddate > '2020-03-20'\n",
    "    order by fileddate asc),\n",
    "\n",
    "     group_by_week as (\n",
    "         select week_filed                                        as first_day_of_week,\n",
    "                count(*) filter (where week_filed is not null)    as cases_filed,\n",
    "                count(*) filter (where week_disposed is not null) as cases_disposed\n",
    "         from after_lockdown\n",
    "         group by week_filed\n",
    "         order by week_filed)\n",
    "\n",
    "select first_day_of_week,\n",
    "       cases_filed,\n",
    "       cases_disposed,\n",
    "       sum(cases_filed) over (order by first_day_of_week)      as cumulative_cases_filed,\n",
    "       sum(cases_disposed) over (order by first_day_of_week)   as cumulative_cases_disposed,\n",
    "       (sum(cases_filed) over (order by first_day_of_week) -\n",
    "        sum(cases_disposed) over (order by first_day_of_week)) as active_cases\n",
    "from group_by_week\n",
    "\"\"\"\n",
    "\n",
    "eviction_cases_time_and_summary = spark.sql(query_eviction_cases_time_and_summary).show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 17,
>>>>>>> Stashed changes
   "id": "final-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### time serise data visz #########################\n",
    "# TODO:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-usage",
   "metadata": {},
   "source": [
    "## Case by zip code"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 18,
>>>>>>> Stashed changes
   "id": "assigned-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Case by zip code  #########################\n",
    "query_eviction_cases_by_zipcode = \"\"\"\n",
    "with cases_zip as (select substr(postalcode, 1, 5) as zip_code\n",
    "                   from eviction_cases\n",
    "                            left join eviction_addresses on eviction_cases.indexnumberid = eviction_addresses.indexnumberid\n",
    "                   where classification in ('Holdover', 'Non-Payment')\n",
    "                     and court in ('Bronx County Civil Court',\n",
    "                                   'Kings County Civil Court',\n",
    "                                   'New York County Civil Court',\n",
    "                                   'Queens County Civil Court',\n",
    "                                   'Richmond County Civil Court',\n",
    "                                   'Redhook Community Justice Center',\n",
    "                                   'Harlem Community Justice Center')\n",
    "                     and fileddate > '2020-03-20'\n",
    "                     and postalcode is not null\n",
    "                     and cast(substr(postalcode, 1, 5) as int) > 1\n",
    "                     and cast(substr(postalcode, 1, 5) as int) < 20000\n",
    "                   order by fileddate asc)\n",
    "\n",
    "select zip_code,\n",
    "       count(*) as total\n",
    "from cases_zip\n",
    "group by zip_code\n",
    "order by zip_code\n",
    "\"\"\"\n",
    "\n",
    "eviction_cases_by_zipcode = spark.sql(query_eviction_cases_by_zipcode).show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 19,
>>>>>>> Stashed changes
   "id": "novel-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### case by zip code data visz #########################\n",
    "# TODO:\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 20,
>>>>>>> Stashed changes
   "id": "valid-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
